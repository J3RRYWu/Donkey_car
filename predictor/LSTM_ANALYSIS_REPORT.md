# ğŸ” LSTMé¢„æµ‹å™¨æ·±åº¦åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**æ•´ä½“è¯„ä»·**: â­â­â­â­ è®¾è®¡è‰¯å¥½ï¼Œä½†æœ‰ä¸€äº›éœ€è¦æ³¨æ„çš„é—®é¢˜

**æ ¸å¿ƒä¼˜åŠ¿**:
- âœ… æ¶æ„è®¾è®¡åˆç†ï¼ˆVAE + LSTMï¼‰
- âœ… æ”¯æŒteacher forcingå’Œopen loop
- âœ… å®Œæ•´çš„è¯„ä¼°æ¡†æ¶
- âœ… æ”¯æŒaction conditioning

**æ½œåœ¨é—®é¢˜**:
- âš ï¸ Teacher forcingå®ç°ä¸­å­˜åœ¨é€»è¾‘æ¼æ´
- âš ï¸ Open loop rolloutçš„èµ·å§‹ç‚¹å¯èƒ½ä¸ä¸€è‡´
- âš ï¸ Actionç´¢å¼•å¯¹é½éœ€è¦ä»”ç»†æ£€æŸ¥
- âš ï¸ ç¼ºå°‘æ˜¾å¼çš„æ›å…‰åå·®ï¼ˆExposure Biasï¼‰å¤„ç†

---

## ğŸ” å‘ç°çš„é—®é¢˜

### âŒ é—®é¢˜1ï¼šTeacher Forcingçš„å®ç°é€»è¾‘æœ‰è¯¯

**ä½ç½®**: `vae_predictor.py`, è¡Œ822-863

**å½“å‰å®ç°**:
```python
teacher_forcing = (target_offset == 1 and T_tgt == T_in)

if teacher_forcing:
    actions_seq = None
    if actions_full is not None:
        actions_seq = actions_full[:, 0:T_in, :]  # âœ… æ­£ç¡®
    z_pred_seq = model.predict(z_input, actions_seq)  # âš ï¸ é—®é¢˜åœ¨è¿™é‡Œ
```

**é—®é¢˜åˆ†æ**:
```python
# model.predict() çš„å®ç°ï¼ˆè¡Œ273-365ï¼‰
def predict(self, z: torch.Tensor, a: Optional[torch.Tensor] = None):
    """
    Input z: (B, T, C, H, W) - æ•´ä¸ªåºåˆ—
    LSTMå¤„ç†: lstm(z_flat) -> è¾“å‡º (B, T, hidden)
    """
    if z_flat.dim() == 2:
        z_flat = z_flat.unsqueeze(1)
    out, _ = self.lstm(z_flat)  # âš ï¸ ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—
```

**é—®é¢˜**:
- `model.predict()` å°†**æ•´ä¸ªè¾“å…¥åºåˆ—**ä¸€æ¬¡æ€§å–‚ç»™LSTM
- è¿™æ„å‘³ç€LSTMåœ¨tæ—¶åˆ»å¯ä»¥"çœ‹åˆ°"t+1, t+2, ...çš„è¾“å…¥
- **è¿™ä¸æ˜¯çœŸæ­£çš„teacher forcingï¼**

**çœŸæ­£çš„teacher forcingåº”è¯¥æ˜¯**:
```python
# ä¼ªä»£ç 
for t in range(T-1):
    z_pred[t+1] = lstm(z_true[t], action[t])  # ç”¨çœŸå®çš„z[t]é¢„æµ‹z[t+1]
```

**ä½†ä½ çš„å®ç°æ˜¯**:
```python
# ä¼ªä»£ç 
z_pred = lstm(z_true[0:T], actions[0:T])  # LSTMå¯ä»¥çœ‹åˆ°æœªæ¥çš„çœŸå®çŠ¶æ€ï¼
```

**å½±å“**:
- è®­ç»ƒæ—¶LSTM"ä½œå¼Š"äº†ï¼ˆçŸ¥é“æœªæ¥ï¼‰
- å¯¼è‡´è®­ç»ƒå’Œæµ‹è¯•çš„gapæ›´å¤§
- å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆè®­ç»ƒåºåˆ—

---

### âš ï¸ é—®é¢˜2ï¼šOpen Loop Rolloutçš„èµ·å§‹ç‚¹ä¸ä¸€è‡´

**ä½ç½®**: `vae_predictor.py`, è¡Œ864-938

**å½“å‰å®ç°**:
```python
# Teacher forcingè·¯å¾„
z_pred_seq = model.predict(z_input, actions_seq)  # é¢„æµ‹æ•´ä¸ªåºåˆ—
# ç„¶åè®¡ç®— loss(z_pred_seq, z_target_seq)

# Open loopè·¯å¾„
z_start = z_input[:, start_idx, ...]  # ä»start_idxå¼€å§‹
for step in range(rollout_steps):
    z_next_pred = model.predict(z_rollout_expanded, a_step)
    z_rollout = z_next_pred.detach()  # è‡ªå›å½’
```

**é—®é¢˜**:
1. **Teacher forcingå’Œopen loopä½¿ç”¨ä¸åŒçš„èµ·å§‹çŠ¶æ€**
   - Teacher forcing: ä½¿ç”¨z_input[:, 0:T_in]çš„æ‰€æœ‰å¸§
   - Open loop: ä»…ä»z_input[:, start_idx]å¼€å§‹

2. **ä¸¤ä¸ªæŸå¤±ä¸æ˜¯çœŸæ­£çš„äº’è¡¥å…³ç³»**
   - Teacher forcing loss: åŸºäº"çœ‹åˆ°æœªæ¥"çš„é¢„æµ‹
   - Open loop loss: åŸºäºè‡ªå›å½’çš„é¢„æµ‹
   - ä¸¤è€…çš„å­¦ä¹ ä¿¡å·å¯èƒ½å†²çª

---

### âš ï¸ é—®é¢˜3ï¼šActionç´¢å¼•å¯¹é½å¯èƒ½æœ‰è¯¯

**ä½ç½®**: `vae_predictor.py`, è¡Œ887-908

**å½“å‰å®ç°**:
```python
# Open loop rollout
actions_rollout = actions_full[:, a0:a1, :]  # a0 = start_idx

for step in range(rollout_steps):
    idx = min(step, actions_rollout.shape[1] - 1)
    a_step = actions_rollout[:, idx:idx+1, :]
    z_next_pred = model.predict(z_rollout_expanded, a_step)
```

**é—®é¢˜**:
- `a_step = actions_rollout[:, step, :]` ä½¿ç”¨ç›¸å¯¹ç´¢å¼•
- ä½†rolloutä»`start_idx`å¼€å§‹ï¼Œactionåº”è¯¥ä¹Ÿä»`start_idx`å¯¹é½
- å¦‚æœ`start_idx > 0`ï¼Œactionç´¢å¼•å¯èƒ½é”™ä½

**ç¤ºä¾‹**:
```
frames:  [f0, f1, f2, f3, f4, f5, f6, f7]
actions: [a0, a1, a2, a3, a4, a5, a6, a7]

context: [f0, f1, f2, f3]  (T_in=4)
target:  [f5, f6, f7]      (target_offset=5)
start_idx = 3 (f3 -> f5çš„å‰ä¸€å¸§)

æ­£ç¡®: z3 + a3 -> z4 -> z5 (ä½†f4ä¸åœ¨åºåˆ—ä¸­)
     z4 + a4 -> z5
     z5 + a5 -> z6
     z6 + a6 -> z7

ä½ çš„å®ç°: 
     actions_rollout = actions[3:6] = [a3, a4, a5]
     step=0: a3 âœ…
     step=1: a4 âœ…
     step=2: a5 âœ…
```

è¿™ä¸ª**çœ‹èµ·æ¥æ˜¯å¯¹çš„**ï¼Œä½†éœ€è¦åœ¨ä¸åŒçš„`target_offset`é…ç½®ä¸‹ä»”ç»†éªŒè¯ã€‚

---

### âš ï¸ é—®é¢˜4ï¼šResidual Connectionçš„é€»è¾‘å¤æ‚

**ä½ç½®**: `vae_predictor.py`, è¡Œ334-365

**å½“å‰å®ç°**:
```python
if self.residual_prediction:
    # Strip actions if present
    base = original_input_flat
    if base.size(-1) > self.latent_flat_dim:
        base = base[..., :self.latent_flat_dim]
    
    if out.dim() == 3:  # Sequence
        if base.dim() == 3 and base.size(1) == out.size(1):
            out = out + base
        elif base.dim() == 2:  # âš ï¸ å•ä¸ªçŠ¶æ€ + åºåˆ—è¾“å‡º
            B = out.size(0)
            T = out.size(1)
            base_seq = base.unsqueeze(1).expand(B, T, -1)
            out = out + base_seq
```

**é—®é¢˜**:
- å¦‚æœ`base.dim() == 2`ï¼ˆå•ä¸ªçŠ¶æ€ï¼‰ä½†`out.dim() == 3`ï¼ˆåºåˆ—ï¼‰
- å½“å‰å®ç°å°†**åŒä¸€ä¸ªbaseåŠ åˆ°æ‰€æœ‰æ—¶é—´æ­¥**
- è¿™å¯èƒ½ä¸æ˜¯ä½ æƒ³è¦çš„

**åº”è¯¥æ˜¯**:
```python
# Residualåº”è¯¥æ˜¯: z_{t+1} = z_t + f(z_t, a_t)
# å¯¹äºåºåˆ—é¢„æµ‹:
z_pred[:, 0] = z_input[:, 0] + f(z_input[:, 0], a[:, 0])  # é¢„æµ‹t=1
z_pred[:, 1] = z_input[:, 1] + f(z_input[:, 1], a[:, 1])  # é¢„æµ‹t=2
...

# è€Œä¸æ˜¯:
z_pred[:, :] = z_input[:, 0] + f(z_input[:, :], a[:, :])  # âŒ
```

---

### âš ï¸ é—®é¢˜5ï¼šExposure Biasæœªæ˜¾å¼å¤„ç†

**èƒŒæ™¯**: 
- Teacher forcingè®­ç»ƒï¼šæ¨¡å‹æ€»æ˜¯çœ‹åˆ°çœŸå®çš„å†å²
- æµ‹è¯•æ—¶ï¼šæ¨¡å‹åªèƒ½çœ‹åˆ°è‡ªå·±çš„é¢„æµ‹
- **è¿™ç§train-testä¸åŒ¹é…ç§°ä¸ºExposure Bias**

**å½“å‰å®ç°**:
```python
# åªæœ‰ä¸¤ç§æ¨¡å¼:
# 1. Teacher forcing (è®­ç»ƒæ—¶)
# 2. Full rollout (æµ‹è¯•æ—¶æˆ–open_loop_loss)

# ç¼ºå°‘æ¸è¿›å¼è¿‡æ¸¡ï¼ˆScheduled Samplingï¼‰
```

**å»ºè®®**: æ·»åŠ Scheduled Sampling
```python
# ä¼ªä»£ç 
for t in range(T-1):
    if random() < schedule(epoch):  # é€æ¸å‡å°‘
        z_input = z_true[t]  # Teacher forcing
    else:
        z_input = z_pred[t]  # ä½¿ç”¨é¢„æµ‹
    z_pred[t+1] = lstm(z_input, a[t])
```

---

### âš ï¸ é—®é¢˜6ï¼šHidden Stateçš„ç®¡ç†ä¸ä¸€è‡´

**ä½ç½®**: `vae_predictor.py`, å¤šå¤„

**é—®é¢˜1: `predict()`ä¸è¿”å›hidden state**
```python
def predict(self, z, a):
    if self.predictor_type == "lstm":
        out, _ = self.lstm(z_flat)  # âŒ hidden stateè¢«ä¸¢å¼ƒ
```

**é—®é¢˜2: `rollout_from_context()`æœ‰hidden stateç®¡ç†**
```python
def rollout_from_context(self, z_context, steps, ...):
    hidden = None  # âœ… æ­£ç¡®åˆå§‹åŒ–
    for t in range(ctx_act_len):
        y, hidden = self._rnn_step(x_in, hidden)  # âœ… æŒç»­æ›´æ–°
```

**ä¸ä¸€è‡´æ€§**:
- `predict()`: batch modeï¼Œæ¯ä¸ªbatchç‹¬ç«‹ï¼ˆæ— çŠ¶æ€ï¼‰
- `rollout_from_context()`: sequential modeï¼Œä¿æŒhidden state

**è¿™æœ¬èº«ä¸æ˜¯bug**ï¼Œä½†å¯èƒ½å¯¼è‡´ï¼š
- `predict()`ç”¨äºteacher forcingæ—¶ï¼ŒLSTMæ¯æ¬¡éƒ½ä»é›¶åˆå§‹åŒ–
- `rollout_from_context()`ç”¨äºrolloutæ—¶ï¼ŒLSTMä¿æŒè¿ç»­æ€§
- ä¸¤ç§æ¨¡å¼çš„è¡Œä¸ºå·®å¼‚å¯èƒ½å½±å“è®­ç»ƒ

---

### âš ï¸ é—®é¢˜7ï¼šLSTMè¾“å…¥ç»´åº¦æ£€æŸ¥ä¸è¶³

**ä½ç½®**: `vae_predictor.py`, è¡Œ174-182

**å½“å‰å®ç°**:
```python
predictor_input_dim = self.latent_flat_dim + action_dim
self.lstm = nn.LSTM(input_size=predictor_input_dim, ...)
```

**é—®é¢˜**:
- å¦‚æœ`action_dim`åœ¨checkpointå’Œå½“å‰ä»£ç ä¸ä¸€è‡´
- LSTMæƒé‡ç»´åº¦ä¼šä¸åŒ¹é…
- ä½†è¿™ä¸ªåœ¨åŠ è½½checkpointæ—¶æ‰ä¼šæŠ¥é”™

**å»ºè®®**: åœ¨`__init__`æ·»åŠ æ–­è¨€
```python
assert self.latent_flat_dim > 0, "latent_flat_dim must be positive"
assert self.action_dim >= 0, "action_dim must be non-negative"
```

---

### âš ï¸ é—®é¢˜8ï¼šOpen Loop Losså¯èƒ½ä¸º0

**ä½ç½®**: `vae_predictor.py`, è¡Œ1089-1095

**å½“å‰å®ç°**:
```python
if open_loop_steps > 0 and open_loop_weight > 0:
    open_loop_val = open_loop_loss.item()
    if open_loop_val > 1e-8:  # åªç»Ÿè®¡éé›¶loss
        total_open_loop += open_loop_val
        num_open_loop_batches += 1
```

**é—®é¢˜**:
- å¦‚æœ`rollout_steps = 0`ï¼ˆå› ä¸º`target_offset`è®¾ç½®ï¼‰ï¼Œ`open_loop_loss`æ°¸è¿œæ˜¯0
- å¯¼è‡´`avg_open_loop = 0`ï¼Œç”¨æˆ·å¯èƒ½è¯¯ä»¥ä¸ºlossè¢«è®¡ç®—äº†
- åº”è¯¥åœ¨é…ç½®ä¸å…¼å®¹æ—¶ç»™å‡ºè­¦å‘Š

---

## ğŸ¯ å…³é”®é€»è¾‘æµç¨‹åˆ†æ

### è®­ç»ƒæµç¨‹

```
1. æ•°æ®åŠ è½½
   â”œâ”€â”€ input_frames:  [f0, f1, f2, ..., f_{T_in-1}]
   â”œâ”€â”€ target_frames: [f_{offset}, f_{offset+1}, ..., f_{offset+T_tgt-1}]
   â””â”€â”€ actions_full:  [a0, a1, a2, ..., a_{L-1}]

2. ç¼–ç 
   â”œâ”€â”€ z_input  = VAE.encode(input_frames)   # (B, T_in, C, H, W)
   â””â”€â”€ z_target = VAE.encode(target_frames)  # (B, T_tgt, C, H, W)

3. Teacher Forcingæ¨¡å¼ (å½“target_offset==1ä¸”T_tgt==T_in)
   â”œâ”€â”€ z_pred = model.predict(z_input, actions[:T_in])  # âš ï¸ é—®é¢˜
   â”œâ”€â”€ loss_tf = MSE(z_pred, z_target)
   â””â”€â”€ âš ï¸ LSTMä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—ï¼Œå¯ä»¥"çœ‹åˆ°æœªæ¥"

4. Open Loopæ¨¡å¼ (å¦‚æœopen_loop_steps>0)
   â”œâ”€â”€ z_cur = z_input[:, start_idx]  # èµ·å§‹çŠ¶æ€
   â”œâ”€â”€ for step in range(rollout_steps):
   â”‚   â”œâ”€â”€ z_cur = model.predict(z_cur, a[step])  # è‡ªå›å½’
   â”‚   â””â”€â”€ loss += MSE(z_cur, z_target[step])
   â””â”€â”€ loss_ol = mean(rollout_losses)

5. æ€»Loss
   â””â”€â”€ loss = loss_tf + open_loop_weight * loss_ol
```

### æ½œåœ¨é—®é¢˜ç‚¹

1. **Teacher forcingä¸æ˜¯é€æ­¥çš„**
   - å½“å‰: `LSTM(z[0:T])` - å¹¶è¡Œå¤„ç†
   - åº”è¯¥: `for t: z[t+1] = LSTM(z[t], hidden)`

2. **ä¸¤ç§æ¨¡å¼çš„èµ·ç‚¹ä¸åŒ**
   - TF: ä»åºåˆ—å¼€å¤´
   - OL: ä»`start_idx`

3. **Hidden stateç®¡ç†ä¸ä¸€è‡´**
   - TF: æ— çŠ¶æ€ï¼ˆæ¯ä¸ªbatché‡ç½®ï¼‰
   - OL: æœ‰çŠ¶æ€ï¼ˆè¿ç»­ï¼‰

---

## âœ… åšå¾—å¥½çš„åœ°æ–¹

### 1. VAEå’ŒLSTMçš„è§£è€¦

```python
# âœ… VAEå†»ç»“ï¼Œåªè®­ç»ƒLSTM
if self.freeze_vae:
    for param in self.vae_encoder.parameters():
        param.requires_grad = False
```

**ä¼˜ç‚¹**:
- VAEå·²ç»è®­ç»ƒå¥½ï¼Œä¸éœ€è¦é‡æ–°å­¦ä¹ é‡å»º
- åªéœ€è¦å­¦ä¹ latent spaceçš„åŠ¨æ€
- è®­ç»ƒæ›´ç¨³å®šæ›´å¿«

### 2. æ”¯æŒä¸åŒçš„åºåˆ—é…ç½®

```python
# âœ… çµæ´»çš„input/targeté…ç½®
input_length = 15       # è¾“å…¥å¸§æ•°
target_length = 15      # ç›®æ ‡å¸§æ•°
target_offset = 1       # ç›®æ ‡èµ·å§‹ä½ç½®
```

**ä¼˜ç‚¹**:
- å¯ä»¥å®ç°next-step predictionï¼ˆoffset=1ï¼‰
- å¯ä»¥å®ç°future chunk predictionï¼ˆoffset=input_lengthï¼‰
- æ”¯æŒå„ç§æ—¶åºä»»åŠ¡

### 3. Action Conditioning

```python
# âœ… æ”¯æŒactionè¾“å…¥
z_flat = torch.cat([z_flat, a], dim=-1)
```

**ä¼˜ç‚¹**:
- å¯ä»¥å­¦ä¹ actionå¯¹æœªæ¥çŠ¶æ€çš„å½±å“
- å¯¹äºcontrolä»»åŠ¡éå¸¸é‡è¦

### 4. æ®‹å·®è¿æ¥

```python
# âœ… æ®‹å·®é¢„æµ‹
if self.residual_prediction:
    out = out + base
```

**ä¼˜ç‚¹**:
- åªéœ€è¦å­¦ä¹ å˜åŒ–é‡ï¼ˆdeltaï¼‰
- è®­ç»ƒæ›´å®¹æ˜“æ›´ç¨³å®š
- é€‚åˆå¹³æ»‘çš„åŠ¨æ€ç³»ç»Ÿ

### 5. Monte Carlo Dropoutä¸ç¡®å®šæ€§ä¼°è®¡

```python
# âœ… MC Dropout
def predict_mc(self, z, a, mc_samples=20):
    for _ in range(mc_samples):
        preds.append(self.predict(z, a))
    return {"mean": mean, "std": std}
```

**ä¼˜ç‚¹**:
- æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡
- å¯¹äºå®‰å…¨å…³é”®ç³»ç»Ÿå¾ˆé‡è¦

### 6. å®Œæ•´çš„rolloutåŠŸèƒ½

```python
# âœ… rollout_from_context
def rollout_from_context(self, z_context, steps, a_full, ...):
    # ä»contexté¢„çƒ­hidden state
    # ç„¶åè‡ªå›å½’é¢„æµ‹Næ­¥
```

**ä¼˜ç‚¹**:
- æ”¯æŒé•¿æœŸé¢„æµ‹
- Hidden stateç®¡ç†æ­£ç¡®
- é€‚åˆMPCç­‰åº”ç”¨

---

## ğŸ”§ å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: çœŸæ­£çš„Teacher Forcing

```python
def predict_with_teacher_forcing(self, z_seq, a_seq=None):
    """
    çœŸæ­£çš„teacher forcing: é€æ­¥é¢„æµ‹
    z_seq: (B, T, ...) çœŸå®latentåºåˆ—
    è¿”å›: z_pred_seq (B, T-1, ...) é¢„æµ‹åºåˆ—ï¼ˆæ¯”è¾“å…¥å°‘1æ­¥ï¼‰
    """
    B, T = z_seq.shape[:2]
    z_flat, _ = self._flatten_latent(z_seq)  # (B, T, D)
    
    hidden = None
    predictions = []
    
    for t in range(T - 1):
        # ä½¿ç”¨çœŸå®çš„z[t]é¢„æµ‹z[t+1]
        x_in = z_flat[:, t, :]
        if a_seq is not None:
            x_in = torch.cat([x_in, a_seq[:, t, :]], dim=-1)
        
        # å•æ­¥é¢„æµ‹
        y, hidden = self._rnn_step(x_in, hidden)
        
        # æ®‹å·®
        if self.residual_prediction:
            y = y + z_flat[:, t, :]
        
        predictions.append(y)
    
    # (B, T-1, D)
    return torch.stack(predictions, dim=1)
```

**ä½¿ç”¨**:
```python
# åœ¨train_epochä¸­
if teacher_forcing:
    z_pred_seq = model.predict_with_teacher_forcing(z_input, actions_seq)
    loss = MSE(z_pred_seq, z_target[:, 1:, ...])  # æ³¨æ„ç´¢å¼•å¯¹é½
```

---

### ä¿®å¤2: Scheduled Sampling

```python
def predict_with_scheduled_sampling(self, z_seq, a_seq=None, schedule_prob=0.5):
    """
    Scheduled sampling: é€æ¸å‡å°‘teacher forcing
    schedule_prob: ä½¿ç”¨çœŸå®zçš„æ¦‚ç‡ï¼ˆ1.0=çº¯TF, 0.0=çº¯rolloutï¼‰
    """
    B, T = z_seq.shape[:2]
    z_flat, _ = self._flatten_latent(z_seq)
    
    hidden = None
    predictions = []
    z_prev = z_flat[:, 0, :]  # èµ·å§‹çŠ¶æ€
    
    for t in range(T - 1):
        # å†³å®šä½¿ç”¨çœŸå®zè¿˜æ˜¯é¢„æµ‹z
        if torch.rand(1).item() < schedule_prob:
            x_in = z_flat[:, t, :]  # ä½¿ç”¨çœŸå®zï¼ˆTFï¼‰
        else:
            x_in = z_prev  # ä½¿ç”¨é¢„æµ‹zï¼ˆrolloutï¼‰
        
        if a_seq is not None:
            x_in = torch.cat([x_in, a_seq[:, t, :]], dim=-1)
        
        y, hidden = self._rnn_step(x_in, hidden)
        
        if self.residual_prediction:
            y = y + x_in[..., :self.latent_flat_dim]
        
        predictions.append(y)
        z_prev = y.detach()  # ç”¨äºä¸‹ä¸€æ­¥
    
    return torch.stack(predictions, dim=1)
```

**è®­ç»ƒæ—¶åŠ¨æ€è°ƒæ•´**:
```python
# åœ¨train.pyä¸­
epoch_progress = epoch / max_epochs
schedule_prob = 1.0 - 0.5 * epoch_progress  # ä»1.0->0.5
```

---

### ä¿®å¤3: ç»Ÿä¸€çš„èµ·å§‹ç‚¹

```python
# åœ¨train_epochä¸­
# Teacher forcingå’Œopen loopä½¿ç”¨ç›¸åŒçš„context
z_context = z_input[:, :context_len, ...]  # ç»Ÿä¸€çš„context
z_start = z_context[:, -1, ...]  # ç»Ÿä¸€çš„èµ·å§‹ç‚¹

# Teacher forcing (ç”¨contexté¢„çƒ­)
z_pred_tf = model.rollout_from_context(
    z_context, steps=T_tgt, a_full=actions, 
    teacher_forcing=True  # æ–°å‚æ•°
)

# Open loop (ç”¨ç›¸åŒçš„contextå’Œèµ·ç‚¹)
z_pred_ol = model.rollout_from_context(
    z_context, steps=T_tgt, a_full=actions,
    teacher_forcing=False
)
```

---

### ä¿®å¤4: Actionå¯¹é½éªŒè¯

```python
# åœ¨DataLoaderä¸­æ·»åŠ éªŒè¯
def verify_action_alignment(frames, actions, target_offset):
    """éªŒè¯actionå’Œframeçš„å¯¹é½"""
    T_frames = len(frames)
    T_actions = len(actions)
    
    # Actionåº”è¯¥æ˜¯transition: a[t] for f[t] -> f[t+1]
    assert T_actions == T_frames - 1, \
        f"Actions ({T_actions}) should be frames-1 ({T_frames-1})"
    
    # å¯¹äºtarget_offsetï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„action
    if target_offset > 0:
        assert target_offset < T_actions, \
            f"target_offset ({target_offset}) >= actions ({T_actions})"
```

---

### ä¿®å¤5: æ·»åŠ é…ç½®éªŒè¯

```python
# åœ¨train_predictor.pyçš„main()ä¸­
def validate_config(args):
    """éªŒè¯è®­ç»ƒé…ç½®çš„ä¸€è‡´æ€§"""
    warnings = []
    
    # 1. Teacher forcingè¦æ±‚
    if args.target_offset == 1:
        if args.target_length != args.input_length:
            warnings.append(
                f"âš ï¸ target_offset=1 (TF) but target_length ({args.target_length}) "
                f"!= input_length ({args.input_length}). "
                f"Teacher forcing may not work as expected."
            )
    
    # 2. Open loopè¦æ±‚
    if args.open_loop_steps > 0:
        max_steps = min(args.target_length, args.sequence_length - args.target_offset)
        if args.open_loop_steps > max_steps:
            warnings.append(
                f"âš ï¸ open_loop_steps ({args.open_loop_steps}) > "
                f"max possible steps ({max_steps}). Will be clipped."
            )
    
    # 3. Actioné…ç½®
    if args.use_actions and args.action_dim == 0:
        warnings.append("âš ï¸ use_actions=True but action_dim=0. Actions will be ignored.")
    
    if warnings:
        print("\n" + "="*60)
        print("Configuration Warnings:")
        for w in warnings:
            print(w)
        print("="*60 + "\n")
```

---

## ğŸ“Š æµ‹è¯•å»ºè®®

### æµ‹è¯•1: Teacher ForcingéªŒè¯

```python
def test_teacher_forcing():
    """æµ‹è¯•TFæ˜¯å¦çœŸçš„é€æ­¥é¢„æµ‹"""
    model = VAEPredictor(...)
    z = torch.randn(2, 10, 64, 4, 4)  # (B, T, C, H, W)
    
    # æ–¹æ³•1: å¹¶è¡Œpredictï¼ˆå½“å‰å®ç°ï¼‰
    z_pred_parallel = model.predict(z)
    
    # æ–¹æ³•2: é€æ­¥predict
    z_pred_seq = []
    hidden = None
    for t in range(9):
        z_t = z[:, t:t+1, ...]
        y, hidden = model._rnn_step(z_t.flatten(1), hidden)
        z_pred_seq.append(y)
    z_pred_sequential = torch.stack(z_pred_seq, dim=1)
    
    # æ£€æŸ¥æ˜¯å¦ç›¸åŒ
    print("Difference:", (z_pred_parallel - z_pred_sequential).abs().max())
    # å¦‚æœå·®å¼‚å¾ˆå¤§ -> è¯´æ˜ä¸¤ç§æ–¹å¼ä¸ç­‰ä»·
```

### æµ‹è¯•2: Actionå¯¹é½éªŒè¯

```python
def test_action_alignment():
    """æµ‹è¯•actionæ˜¯å¦æ­£ç¡®å¯¹é½"""
    dataset = TrajectoryDataset(...)
    batch = next(iter(DataLoader(dataset, batch_size=1)))
    
    input_frames, target_frames, actions = batch
    
    # æ‰“å°ä¿¡æ¯
    print(f"Input: frames [0:{len(input_frames[0])}]")
    print(f"Target: frames [{target_offset}:{target_offset + len(target_frames[0])}]")
    print(f"Actions: actions [0:{len(actions[0])}]")
    
    # éªŒè¯: 
    # - action[t] åº”è¯¥ç”¨äº frame[t] -> frame[t+1]
    # - å¯¹äºtarget_frames[k]ï¼Œåº”è¯¥ä½¿ç”¨action[target_offset-1+k]
```

### æµ‹è¯•3: Rolloutè¿ç»­æ€§

```python
def test_rollout_continuity():
    """æµ‹è¯•rolloutæ˜¯å¦è¿ç»­"""
    model = VAEPredictor(...)
    z_context = torch.randn(1, 5, 64, 4, 4)
    
    # ä¸€æ¬¡rollout 10æ­¥
    z_rollout_10 = model.rollout_from_context(z_context, steps=10)
    
    # åˆ†ä¸¤æ¬¡: 5+5æ­¥
    z_rollout_5a = model.rollout_from_context(z_context, steps=5)
    # æ¥ç€å†rollout 5æ­¥ï¼ˆéœ€è¦ä¿®æ”¹APIä»¥æ¥å—hidden stateï¼‰
    z_rollout_5b = model.rollout_from_context(
        torch.cat([z_context, z_rollout_5a], dim=1), 
        steps=5
    )
    
    # æ£€æŸ¥æœ€å5æ­¥æ˜¯å¦ä¸€è‡´
    print("Difference:", (z_rollout_10[:, 5:] - z_rollout_5b).abs().max())
```

---

## ğŸ¯ ä¼˜å…ˆçº§å»ºè®®

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

1. **ä¿®å¤Teacher Forcingé€»è¾‘** â­â­â­
   - å½“å‰å®ç°ä¸æ˜¯çœŸæ­£çš„TF
   - ä¼šå¯¼è‡´train-test gap
   - å½±å“ï¼šå‡†ç¡®ç‡å¯èƒ½è™šé«˜

2. **ç»Ÿä¸€TFå’ŒOLçš„èµ·å§‹ç‚¹** â­â­â­
   - ä¸¤ä¸ªlossçš„èµ·ç‚¹åº”è¯¥ä¸€è‡´
   - å½“å‰ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´æ··ä¹±çš„æ¢¯åº¦
   - å½±å“ï¼šè®­ç»ƒä¸ç¨³å®š

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®ä¿®å¤ï¼‰

3. **æ·»åŠ Scheduled Sampling** â­â­
   - ç¼“è§£exposure bias
   - æé«˜æ³›åŒ–èƒ½åŠ›
   - å½±å“ï¼šé•¿æœŸé¢„æµ‹å‡†ç¡®ç‡

4. **éªŒè¯Actionå¯¹é½** â­â­
   - ç¡®ä¿actionå’Œframeå¯¹åº”æ­£ç¡®
   - æ·»åŠ æ–­è¨€å’Œæµ‹è¯•
   - å½±å“ï¼šå¦‚æœé”™è¯¯ï¼Œactionå®Œå…¨æ— æ•ˆ

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰æ”¹è¿›ï¼‰

5. **æ”¹è¿›Residual Connection** â­
   - å½“å‰å®ç°åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸å¯¹
   - ä½†å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½å·¥ä½œ

6. **æ·»åŠ é…ç½®éªŒè¯** â­
   - å¸®åŠ©ç”¨æˆ·å‘ç°é…ç½®é”™è¯¯
   - æé«˜å¯ç”¨æ€§

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒé—®é¢˜

**æœ€ä¸¥é‡çš„é—®é¢˜æ˜¯Teacher Forcingçš„å®ç°**:
- å½“å‰å®ç°è®©LSTMä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—
- è¿™æ„å‘³ç€tæ—¶åˆ»å¯ä»¥"çœ‹åˆ°"t+1, t+2, ...çš„çœŸå®çŠ¶æ€
- **è¿™ä¸æ˜¯çœŸæ­£çš„Teacher Forcingï¼**

### å»ºè®®çš„è¡ŒåŠ¨

1. **ç«‹å³**: å®ç°çœŸæ­£çš„é€æ­¥Teacher Forcing
2. **å°½å¿«**: æ·»åŠ æµ‹è¯•éªŒè¯TFã€actionå¯¹é½ã€rollout
3. **ä¹‹å**: è€ƒè™‘æ·»åŠ Scheduled Sampling

### å¥½æ¶ˆæ¯

- æ•´ä½“æ¶æ„æ˜¯å¥å£®çš„
- VAE+LSTMçš„è®¾è®¡æ˜¯åˆç†çš„
- å¤§éƒ¨åˆ†åŠŸèƒ½å®ç°æ­£ç¡®
- ä¿®å¤è¿™äº›é—®é¢˜ç›¸å¯¹ç®€å•

### æœŸæœ›æ”¹è¿›

ä¿®å¤åé¢„æœŸ:
- è®­ç»ƒlosså¯èƒ½ç¨å¾®ä¸Šå‡ï¼ˆå› ä¸ºä¸å†"ä½œå¼Š"ï¼‰
- ä½†**æµ‹è¯•æ€§èƒ½åº”è¯¥æé«˜**
- Train-test gapä¼šç¼©å°
- æ¨¡å‹æ›´robust

---

**çŠ¶æ€**: ğŸŸ¡ éœ€è¦æ”¹è¿›ä½†ä¸æ˜¯è‡´å‘½é—®é¢˜  
**å»ºè®®**: å»ºè®®ä¿®å¤TFé€»è¾‘åå†è¿›è¡Œç”Ÿäº§éƒ¨ç½²  
**æ—¶é—´**: é¢„è®¡2-4å°æ—¶ä¿®å¤æ ¸å¿ƒé—®é¢˜

---
